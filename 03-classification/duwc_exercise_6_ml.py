# -*- coding: utf-8 -*-
"""DUWC_Exercise_6_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZZ6_KiYPdwXs0dbU5ePLecRfcN8dDq0_

# Machine Learning Applications

In this exercise, you will apply several classification techniques to PPG features extracted from a wristband. We read this data into a csv file and each column represents a feature and each row represents a different window. Lets first read the data:
"""

import pandas as pd
y = pd.read_csv("https://megastore.rz.uni-augsburg.de/get/Q4hRAcsopj/")
y=y.dropna()

y

"""The column Change_Cortisolwert_1_to_4 describes the stress level of the participant on a given window. Lets try to classify its value between 0-1 by using different classification algorithms.

# 1. kNN Classification

First lets try kNN classifier from Python library scikit. It is a simple algorithm decides by the label of the closest neighbours.
"""

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import train_test_split

col = "SessionLabel"
data = y.loc[:, y.columns != col]
target=y.loc[:, y.columns == col]

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=False)

# then report the best accuracy and how do you achive it?
neigh = KNeighborsClassifier(n_neighbors=15)# chajnge from 1 - 50?# Find the kNN Classifier algorithm function from scikit and test it with different neighbor numbers between 1-15.
#Report the best mean absolute error value and the number of neighbours. ‚ùì
neigh.fit(X_train, y_train)


from sklearn.metrics import accuracy_score

res = neigh.predict(X_test)
model_error = accuracy_score(y_test, res)
print(f"The accuracy of the optimal model is {model_error:.2f}")
# n_neighbors

"""The best accuracy: 0.78

Corresponding # of neighbours:14

> Add blockquote

# 2. Linear Regression

Linear regression is another simple technique to estimate continuous values.


‚ùì In the below code find and examine the linear regression function from the scikit and run with default parameters.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression



col = "SessionLabel"
data = y.loc[:, y.columns != col]
target=y.loc[:, y.columns == col]

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=False)


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

linear_regression = LinearRegression() # scikit linear regression algorithm goes here !!!!!
linear_regression.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error

res = linear_regression.predict(X_test)
model_error = mean_squared_error(y_test, res)
print(f"The mean squared error of the optimal model is {model_error:.2f}")

"""# 3. Random Forest

Tree based architectures also provide robust performance especially with tabular data. Random Forest Regressor uses collection of trees to predict the continuous value.

‚ùì In the below code, find the random forest classifier from the scikit library and test the maximum depth from 1-10. The other parameters will be default values. Report the best accuracy value and the corresponding max depth.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

col = "SessionLabel"
data = y.loc[:, y.columns != col]
target=y.loc[:, y.columns == col]

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=False)

# change it from 1 to 5 0 and then report how do you aachive it?
regr = RandomForestClassifier(max_depth=15)# Random Forest Regressor function will go here.
regr.fit(X_train, y_train)


from sklearn.metrics import accuracy_score

res = regr.predict(X_test)
model_error = accuracy_score(y_test, res)
print(f"The accuracy the optimal model is {model_error:.2f}")

"""# üßë  ANSWER

The best accuracy: 3, 5, 7

Corresponding max depth: 0.8

# 4. Support Vector Machine

Support Vector machines (SVM) are also powerful and robust tools. They can be used with different kernel functions. In scikit, you can use kernel parameter with the following values: kernel{‚Äòlinear‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô, ‚Äòprecomputed‚Äô} .

‚ùì Below code try all the kernel functions and report the best value and the corresponding kernel type:
"""

import pandas as pd
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

col = "SessionLabel"
data = y.loc[:, y.columns != col]
target=y.loc[:, y.columns == col]

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=False)

# same with this then report it and the best accuracy and how do you achieve it?
regr = make_pipeline(StandardScaler(), SVC(C=1.0))
#regr = make_pipeline(StandardScaler(), SVC(C=1.0, kernel='precomputed'))
regr.fit(X_train, y_train)
Pipeline(steps=[('standardscaler', StandardScaler()),('svr', SVC(C=1.0))])

from sklearn.metrics import accuracy_score

res = regr.predict(X_test)
model_error = accuracy_score(y_test, res)
print(f"The accuracy of the optimal model is {model_error:.2f}")

"""# üßë  ANSWER

The best accuracy:0.80

Corresponding kernel type:linear,poly,rbf

# 5. Neural Networks

Neural Networks achieve promising results in most of the problems. Multilayer Perceptron is one of the simplest Neural Networks. In this exercise, we will use MLP from scikit learn. However, they come with a lot of parameters. Therefore, selecting the best parameters is vital for a good performance. The process is called hyperparameter optimization. Below code, we added some hyperparameter optimization.

‚ùì Run the code and report the accuracies. It can take longer please be patient.
"""

import pandas as pd
from sklearn.neural_network import MLPClassifier,MLPRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate


col = "SessionLabel"
data = y.loc[:, y.columns != col]
target=y.loc[:, y.columns == col]

X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42, shuffle=False)


best_mlp = MLPClassifier(hidden_layer_sizes = (100,100, 1),
                        activation ='relu',
                        solver='adam',
                        max_iter= 100, n_iter_no_change = 20)


scores = cross_validate(best_mlp, X_test, y_test, scoring='accuracy', return_train_score=True, return_estimator = True)
print (scores["test_score"])

regr = MLPRegressor(random_state=1, max_iter=1000, early_stopping=True).fit(X_train, y_train)



from sklearn.metrics import mean_squared_error

inferred_body_mass = regr.predict(data)
model_error = mean_squared_error(target, inferred_body_mass)
print(f"The mean squared error of the optimal model is {model_error:.2f}")

from sklearn.metrics import mean_absolute_error

model_error = mean_absolute_error(target, inferred_body_mass)
print(f"The mean absolute error of the optimal model is {model_error:.2f} ")

